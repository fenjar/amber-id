# AMBER-ID: Avatar Motion-Based Embeddings for Re-Identification
Implementation of the Master Thesis (TU Berlin, 2025)

AMBER-ID (Avatar Motion-Based Embedding for Re-Identification) is an end-to-end framework for evaluating the privacy of avatar-anonymized talking-head videos.
It investigates whether behavioral biometric informationâ€”specifically facial motion dynamicsâ€”can still be exploited to identify individuals, even when all appearance-based cues have been replaced by an avatar.

This repository contains the full implementation used in the thesis â€œAssessing Privacy of Anonymized Videoâ€, including preprocessing, model training, and evaluation pipelines.

## ğŸ” Overview
While avatarization removes physiological biometric information (appearance), it preserves facial motion to maintain expressiveness and usability.
However, motion patterns themselves can contain unique, identity-specific signatures.

AMBER-ID evaluates this privacy risk by training Spatio-Temporal Graph Convolutional Networks (ST-GCNs) on:

- Original talking-head videos â†’ Baseline model

- Avatarized videos (via RAVAS) â†’ Attacker model

and then comparing their verification performance.

## Key Features

âœ”ï¸ ST-GCN-based motion representation learning

âœ”ï¸ training pipeline adjustable to train a Baseline (refM) and Attacker (attM) models 

âœ”ï¸ Complete privacy evaluation suite:

  - Equal Error Rate (EER)

  - ROC & AUC

  - Pairwise cosine similarity distributions

  - t-SNE visualization of the embedding space

âœ”ï¸ Region-based ablation studies (eyes, lips, nose, eyebrows, face oval)

âœ”ï¸ Applicable to any anonymization method or to clear data

âœ”ï¸ Fully reproducible training & evaluation setup

## Method Summary

1ï¸âƒ£ Datasets

The Driving Identity Dataset (DID-D) combines:

- RAVDESS â€” studio-quality emotional speech

- CREMA-D â€” diverse, crowd-sourced expressive speech

- NVIDIA-VS subset â€” scripted & free monologues in video-call conditions

All DID-D videos are processed through RAVAS to produce the Target Avatar Dataset (TAV-D).

2ï¸âƒ£ Feature Extraction

Using MediaPipe FaceMesh:

- 2D facial landmarks (152 points)

- blendshape coefficients

- head pose matrices

are extracted and normalized to a canonical coordinate space.

3ï¸âƒ£ Model â€” ST-GCN

The Spatio-Temporal Graph Convolutional Network:

- models spatial relations between landmarks

- captures temporal dynamics across frames

- is trained using cross-entropy classification

outputs embeddings suitable for 1:1 verification using cosine similarity

4ï¸âƒ£ Evaluation Pipeline

Each model is evaluated on unseen identities using:

- ROC curves & AUC

- Equal Error Rate (EER)

- Accuracy at optimal threshold

- Cosine similarity distributions

- t-SNE embedding visualizations

ğŸ“Š Results (Summary)

Motion alone is sufficient for identity inference â†’ De-anonymization is possible. Avatarization reduces identity leakage â†’ but does not eliminate it.

Best model performance:

- Baseline EER â‰ˆ 0.16

- Attacker EER â‰ˆ 0.26

Most identity-revealing motion regions: Eyes, Lips, Nose

## ğŸš€ Getting Started
