Job 2182786: Running on node(s) serv-3333
Job 2182786: Started at 2025-09-27 00:00:07+0200
Monitor this job here: http://monitoring.pegasus.kl.dfki.de/d/slurm-job-details/job-details?var-jobid=2182786&from=1758924007000
srun: jobinfo: version v1.0.0
Job 2182786: Running on node(s) serv-3333
Job 2182786: Started at 2025-09-27 00:00:19+0200
Monitor this job here: http://monitoring.pegasus.kl.dfki.de/d/slurm-job-details/job-details?var-jobid=2182786&from=1758924019000
Job 2182786: creating container for /netscratch/fschulz/video_deanon_train_cuda.sqsh
Job 2182786: creating container for /netscratch/fschulz/video_deanon_train_cuda.sqsh took 51.2 seconds

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
LOCAL_RANK from env: 0
LOCAL_RANK from env: 0
LOCAL_RANK from env: 0
LOCAL_RANK from env: 2
LOCAL_RANK from env: 2
LOCAL_RANK from env: 3
LOCAL_RANK from env: 1
LOCAL_RANK from env: 1
LOCAL_RANK from env: 0
LOCAL_RANK from env: 1
LOCAL_RANK from env: 3
LOCAL_RANK from env: 2
LOCAL_RANK from env: 3
LOCAL_RANK from env: 3
LOCAL_RANK from env: 2
LOCAL_RANK from env: 1
[rank0] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank2] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank0] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[rank0] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank3] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[rank2] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank1] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank1] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[rank0] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[rank1] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[rank3] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank1] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank3] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[rank2] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank3] CUDA_VISIBLE_DEVICES: 0,1,2,3
[rank2] CUDA_VISIBLE_DEVICES: 0,1,2,3
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
Number of batches per epoch: 7
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924082.916513 1113027 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924082.936335 1113182 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924082.970065 1112970 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924082.994100 1113217 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.001938 1113316 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.013638 1113515 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.033656 1113206 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924083.092726 1113453 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.092936 1113900 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.101970 1113621 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.105697 1113822 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.117004 1113539 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.160144 1113954 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.175957 1113819 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.325063 1113738 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924083.337433 1114004 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s]/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
Number of batches per epoch: 7
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7Number of batches per epoch: 7

Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s]/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
Number of batches per epoch: 7
/home/fschulz/video_deanon_train/train.py:100: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
Number of batches per epoch: 7
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[SAMPLER] Dataset Length: 112
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[SAMPLER] Dataset Length: 112
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
[SAMPLER] Dataset Length: 112
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[SAMPLER] Dataset Length: 112
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
[SAMPLER] Dataset Length: 112
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924084.444583 1114711 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924084.473753 1115001 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924084.485056 1115241 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.487229 1114888 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924084.497707 1115146 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924084.553928 1115538 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924084.586135 1115745 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.587427 1114702 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924084.593720 1115314 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.597307 1114631 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.598813 1115337 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.608007 1114783 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.630527 1114658 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
W0000 00:00:1758924084.658578 1116010 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.674284 1117333 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1758924084.767315 1114955 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.794056 1114497 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.802866 1115476 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.853526 1116019 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.857442 1117020 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.872563 1116331 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.891098 1115913 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.900690 1116081 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.940383 1117736 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.943173 1114509 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924084.979380 1117306 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.004492 1117538 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.024690 1116522 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.025049 1116080 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.035854 1116620 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.037893 1116700 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.038366 1116533 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.040214 1116423 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.049750 1116998 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.087350 1116242 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.094214 1116184 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.125804 1117777 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.150870 1116807 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.165277 1117499 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.197279 1117505 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.203502 1117074 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.205818 1117285 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.266548 1116902 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.317638 1117596 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.365922 1117755 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.372787 1116857 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.503828 1117093 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
W0000 00:00:1758924085.524146 1117805 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id025/q01-id001.mp4: only 32 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id025/q01-id001.mp4 is too short to be processed. Skipping this video.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id059/q03-id035.mp4: only 34 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id059/q03-id035.mp4 is too short to be processed. Skipping this video.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id134/1064_TIE_SAD_XX.mp4: only 18 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/cremad_id134/1064_TIE_SAD_XX.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55d9016c02c0] moov atom not found
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id060/a04-id036.mp4: only 29 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id060/a04-id036.mp4 is too short to be processed. Skipping this video.
[mov,mp4,m4a,3gp,3g2,mj2 @ 0x558508bc9c40] moov atom not found
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id049/a06-id025.mp4.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5637fd9465c0] moov atom not found
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4 is too short to be processed. Skipping this video.
[mov,mp4,m4a,3gp,3g2,mj2 @ 0x5637fb84d300] moov atom not found
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id064/a07-id040.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[TRAIN] Epoch 0 Step 0 Loss 0.9137 GradNorm 73217.9710 {'N': 0.8210582733154297, 'Q': 0.883387565612793, 'R': 0.8188750147819519, 'p': 0.41980233788490295}
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[WARNING] Skipping /netscratch/fschulz/tav-d/train/nvidia_id069/q06-id045.mp4: only 24 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/nvidia_id069/q06-id045.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_NEU_XX.mp4: only 3 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_NEU_XX.mp4 is too short to be processed. Skipping this video.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4: no face detected in any frame.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4: no face detected in any frame.
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/cremad_id146/1076_MTI_SAD_XX.mp4 is too short to be processed. Skipping this video.
[WARNING] Skipping /netscratch/fschulz/tav-d/train/cremad_id144/1074_ITS_DIS_XX.mp4: only 19 valid frames (<35)
[SAMPLER] Warning: /netscratch/fschulz/tav-d/train/cremad_id144/1074_ITS_DIS_XX.mp4 is too short to be processed. Skipping this video.
/home/fschulz/video_deanon_train/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
Epoch 1:   0%|          | 0/7 [12:36<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/fschulz/video_deanon_train/train.py", line 157, in <module>
[rank0]:     main()
[rank0]:   File "/home/fschulz/video_deanon_train/train.py", line 117, in main
[rank0]:     embeddings = model(segments)   # (B, 5, E)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/fschulz/video_deanon_train/model.py", line 60, in forward
[rank0]:     outs.append(self.forward_F(x[:, off:off+self.F, :]))  # (B,E)
[rank0]:   File "/home/fschulz/video_deanon_train/model.py", line 46, in forward_F
[rank0]:     x = self.backbone(x)                 # (B,hidden,F,1,1) with padding->F stays the same
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
[rank0]:     input = module(input)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py", line 193, in forward
[rank0]:     return F.batch_norm(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2817, in batch_norm
[rank0]:     return torch.batch_norm(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 600.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 324.25 MiB is free. Process 1111825 has 59.20 GiB memory in use. Process 1111840 has 1.07 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Process 1111846 has 1.07 GiB memory in use. Of the allocated memory 15.89 GiB is allocated by PyTorch, and 590.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/fschulz/video_deanon_train/train.py", line 157, in <module>
[rank1]:     main()
[rank1]:   File "/home/fschulz/video_deanon_train/train.py", line 117, in main
[rank1]:     embeddings = model(segments)   # (B, 5, E)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/fschulz/video_deanon_train/model.py", line 60, in forward
[rank1]:     outs.append(self.forward_F(x[:, off:off+self.F, :]))  # (B,E)
[rank1]:   File "/home/fschulz/video_deanon_train/model.py", line 46, in forward_F
[rank1]:     x = self.backbone(x)                 # (B,hidden,F,1,1) with padding->F stays the same
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
[rank1]:     input = module(input)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py", line 193, in forward
[rank1]:     return F.batch_norm(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2817, in batch_norm
[rank1]:     return torch.batch_norm(
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 600.00 MiB. GPU 1 has a total capacity of 79.25 GiB of which 236.25 MiB is free. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Process 1111826 has 59.20 GiB memory in use. Process 1111841 has 1.12 GiB memory in use. Process 1111847 has 1.12 GiB memory in use. Of the allocated memory 15.89 GiB is allocated by PyTorch, and 590.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/fschulz/video_deanon_train/train.py", line 157, in <module>
[rank2]:     main()
[rank2]:   File "/home/fschulz/video_deanon_train/train.py", line 117, in main
[rank2]:     embeddings = model(segments)   # (B, 5, E)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/fschulz/video_deanon_train/model.py", line 60, in forward
[rank2]:     outs.append(self.forward_F(x[:, off:off+self.F, :]))  # (B,E)
[rank2]:   File "/home/fschulz/video_deanon_train/model.py", line 46, in forward_F
[rank2]:     x = self.backbone(x)                 # (B,hidden,F,1,1) with padding->F stays the same
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py", line 244, in forward
[rank2]:     input = module(input)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py", line 193, in forward
[rank2]:     return F.batch_norm(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 2817, in batch_norm
[rank2]:     return torch.batch_norm(
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 600.00 MiB. GPU 2 has a total capacity of 79.25 GiB of which 236.25 MiB is free. Process 1111829 has 59.20 GiB memory in use. Including non-PyTorch memory, this process has 17.56 GiB memory in use. Process 1111842 has 1.12 GiB memory in use. Process 1111848 has 1.12 GiB memory in use. Of the allocated memory 15.89 GiB is allocated by PyTorch, and 590.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W927 00:14:01.964622573 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0927 00:14:03.206000 1108811 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111835 closing signal SIGTERM
W0927 00:14:03.208000 1108811 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111836 closing signal SIGTERM
W0927 00:14:03.210000 1108811 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111837 closing signal SIGTERM
E0927 00:14:03.945000 1108811 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 1111834) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 905, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-09-27_00:14:03
  host      : serv-3333.kl.dfki.de
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1111834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: serv-3333: task 1: Exited with exit code 1
srun: Terminating StepId=2182786.0
slurmstepd: error: *** STEP 2182786.0 ON serv-3333 CANCELLED AT 2025-09-27T00:14:04 ***
Epoch 1:  71%|  | 5/7 [12:37<03:10, 95.00s/it] W0927 00:14:04.782000 1108813 torch/distributed/elastic/agent/server/api.py:723] Received Signals.SIGTERM death signal, shutting down workers
W0927 00:14:04.783000 1108813 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111825 closing signal SIGTERM
W0927 00:14:04.781000 1108810 torch/distributed/elastic/agent/server/api.py:723] Received Signals.SIGTERM death signal, shutting down workers
W0927 00:14:04.784000 1108812 torch/distributed/elastic/agent/server/api.py:723] Received Signals.SIGTERM death signal, shutting down workers
W0927 00:14:04.784000 1108813 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111826 closing signal SIGTERM
W0927 00:14:04.785000 1108812 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111846 closing signal SIGTERM
W0927 00:14:04.785000 1108810 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111840 closing signal SIGTERM
W0927 00:14:04.787000 1108812 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111847 closing signal SIGTERM
W0927 00:14:04.793000 1108813 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111829 closing signal SIGTERM
W0927 00:14:04.805000 1108812 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111848 closing signal SIGTERM
W0927 00:14:04.820000 1108813 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111830 closing signal SIGTERM
W0927 00:14:04.824000 1108812 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111849 closing signal SIGTERM
W0927 00:14:04.831000 1108810 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111841 closing signal SIGTERM
W0927 00:14:04.833000 1108810 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111842 closing signal SIGTERM
W0927 00:14:04.833000 1108810 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 1111843 closing signal SIGTERM
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 905, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1108812 got signal: 15
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 905, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1108810 got signal: 15
srun: error: serv-3333: tasks 0,2: Exited with exit code 1
Traceback (most recent call last):
  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 905, in <module>
    main()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    result = agent.run()
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py", line 138, in wrapper
    result = f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 715, in run
    result = self._invoke_run(role)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py", line 879, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 1108813 got signal: 15
srun: error: serv-3333: task 3: Exited with exit code 1
